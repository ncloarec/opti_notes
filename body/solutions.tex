\begin{exo}

 Le but de cet exercice est de montrer \textit{l'inégalité de von Neumann}. Soient
 $A, B \in \R^{m \times k}$ deux matrices. Alors,
 \begin{equation}\label{eq:Neumann}
  \left| \operatorname { Tr } \left( A ^ { \top } B \right) \right| \leq \sum _ { j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { j } ( B )
 \end{equation}
 où $q = \min ( m , k )$ et
 \begin{align}
  { \lambda _ { 1 } ( A ) \geq \lambda _ { 2 } ( A ) \geq
   \cdots \geq \lambda _ { q } ( A ) \geq 0} \label{eq:ord1} \\
  { \lambda _ { 1 } ( B ) \geq \lambda _ { 2 } ( B ) \geq \cdots \geq
  \lambda _ { q } ( B ) \geq 0 }\label{eq:ord2}
 \end{align}
 sont les valeurs singulières de $A$ et $B$.
\end{exo}

\begin{qst}

 En utilisant les SVD de $A$ et $B$, montrer que
 $$\left| \operatorname { Tr } \left( A ^ { \top } B \right) \right| \leq \sum _ { i , j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { i } ( B ) d _ { i j }$$
 où les coefficients $d_{i,j}$ vérifient
 $ \displaystyle \forall i , j : \quad d _ { i j } \geq 0 , \quad
  \sum _ { i = 1 } ^ { q } d _ { i j } \leq 1 , \quad \sum _ { j = 1 } ^
  { q } d _ { i j } \leq 1$.
\end{qst}

\begin{rep}

 Notons $u_j\prt{A}, v_j\prt{A}$ (\textit{respectivement} $u_j\prt{B}, v_j\prt{B}$)
 les vecteurs intervenant dans la SVD de $A$ (\textit{respectivement} $B$).


 En utilisant successivement la linéarité de la trace, l'inégalité triangulaire
 et la positivité des valeurs singulières, on a :
 \begin{align*}
  \left| \operatorname { Tr } \left( A ^ { \top } B \right) \right|
   & =
  \abs{\tr{\prt{\sum_{i,j} \lambda_i\prt{A} \lambda_j\prt{B}v_i\prt{A}
     \transpose{u_i\prt{A}}u_j\prt{B}\transpose{v_j\prt{B}}}}}                            \\
   & \leq \sum_{i,j}\lambda_i\prt{A} \lambda_j\prt{B}\abs{\tr{\prt{ v_i\prt{A}
     \transpose{u_i\prt{A}}u_j\prt{B}\transpose{v_j\prt{B}}}}}                            \\
   & \leq \sum_{i,j}\lambda_i\prt{A} \lambda_j\prt{B} \abs{\bigps{u_i\prt{A}}{u_j\prt{B}}
   \cdot \bigps{v_i\prt{A}}{v_j\prt{B}}}                                                  \\
   & \leq \sum_{i,j}\lambda_i\prt{A} \lambda_j\prt{B} \cdot  d_{i,j}
 \end{align*}
 où $ \displaystyle d_{i,j} = \abs{\bigps{u_i\prt{A}}{u_j\prt{B}}
   \cdot \bigps{v_i\prt{A}}{v_j\prt{B}}} $ et $\bigps{\,\cdot\,}{\,\cdot\,}$ désigne le
   produit scalaire usuel euclidien.

\newpage
 On a clairement $d_{i,j} \geq 0$.

 \begin{itemize}[font= \color{blue} \Large, label= $\bullet$]
  \item Montrons que $ \displaystyle \sum_j d_{i,j} \leq 1$ à $i$ fixé. En utilisant
        l'orthonomarlité des bases $\prt{\,u_j\prt{B}\,}_{1 \leq j \leq q}$ et
        $\prt{\,v_j\prt{B}\,}_{1 \leq j \leq q}$, on a :
        \begin{align*}
         \sum_j d_{i,j} & = \sum_j \abs{\bigps{u_i\prt{A}}{u_j\prt{B}}}
         \cdot \abs{\bigps{v_i\prt{A}}{v_j\prt{B}}}                                                                         \\
                        & \leq \sum_j \frac{1}{2} \acc{\bigps{u_i\prt{A}}{u_j\prt{B}}^2 + \bigps{v_i\prt{A}}{v_j\prt{B}}^2} \\
                        & \leq \frac{1}{2} \acc{\norm{u_i\prt{A}}_2^2+\norm{v_i\prt{A}}_2^2 }                               \\
                        & \leq 1
        \end{align*}
        la dernière inégalité provenant du caractère normé des vecteurs
        $u_i\prt{A}$ et $v_i\prt{A}$.
  \item Pour obtenir l'inégalité en sommant sur $i$ à $j$ fixé, il suffit de
        constater la symétrie de $D = \prt{d_{i,j}}_{1 \leq i,j \leq q}$ provenant de
        celle du produit scalaire.
 \end{itemize}
 Finalement les coefficients de $D$ vérifient bien les inégalités demandées.
\end{rep}

\begin{qst}
 Soit $D$ la matrice avec les éléments $d_{i,j}$. Conclure que, pour montrer
 \eqref{eq:Neumann}, il suffit de pouver l'inégalité
 \begin{equation}\label{eq:D}
  \sum _ { i , j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { i } ( B )
  d _ { i j } - \sum _ { j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { j } ( B ) \leq 0
 \end{equation}
 pour tous $\left( \lambda _ { j } ( A ) \right) _ { j = 1 } ^ { d }$ et
 $\left( \lambda _ { j } ( B ) \right) _ { j = 1 } ^ { d }$ vérifiant
 \eqref{eq:ord1}, \eqref{eq:ord2} et la condition
 $ \displaystyle \lambda _ { 1 } ( A ) = \lambda _ { 1 } ( B ) = 1$.
\end{qst}

\begin{rep}

 Si $A = 0$ ou $B = 0$, l'inégalité est clairement vérifiée (\textit{c'est même un égalité}). Supossons maintenant $A$ et $B$ non nuls. On a dans ce cas :
 $$ \lambda_1\prt{A} >0 \quad \text{ et } \quad \lambda_1\prt{B}>0.$$

 En normalisant les valeurs singulières $\lambda_i\prt{A}$ et $\lambda_j\prt{B}$
 par $\lambda_1\prt{A}$ et $\lambda_1\prt{B}$ respectivement, l'inégalité \eqref{eq:D}
 revient à montrer :
 $$  \sum _ { i , j = 1 } ^ { q } \lambda _ { j }^{\prime} ( A ) \lambda _ { i }^{\prime} ( B )
  d _ { i j } - \sum _ { j = 1 } ^ { q } \lambda _ { j }^{\prime} ( A ) \lambda _ { j }^{\prime} ( B ) \leq 0,$$
 avec $\displaystyle \lambda _ { k }^{\prime}\prt{\, \cdot \, } =
  \frac{\lambda _ { k }\prt{\, \cdot \, }}{\lambda_1\prt{\, \cdot \, }}$ et les
 $\lambda _ { k }^{\prime}$ vérifiant bien les conditions demandées.
\end{rep}

\begin{qst}

 Pour $r \in \acc{1 \ldots q}$, définissons
 $$\Lambda _ { r } = \left\{ \lambda \in \mathbb { R } ^ { q } \; : \;
  \lambda _ { 1 } = 1,1 \geq \lambda _ { 2 } \geq \cdots \geq \lambda _ { r } >
  0 , \; \lambda _ { j } = 0 , \;j > r \right\}.$$

 Montrer que $$\Lambda _ { r } \subseteq \operatorname { conv }
  \left( \mu _ { 1 } , \ldots , \mu _ { r } \right)$$
 où $$\mu _ { j } = ( \underbrace { 1 , \ldots , 1 } _ { j \text { fois }
  } , 0 , \ldots , 0 ) \in \mathbb { R } ^ { q },$$
 et, pour les vecteurs $x _ { 1 } , \dots , x _ { r } \in \mathbb { R } ^ { q }$,
 on désigne par $\conv \left( x _ { 1 } , \dots , x _ { r } \right)$ leur enveloppe
 convexe.
\end{qst}

\begin{rep}

 Montrons le résultat par récurrence forte sur $r$.
 \begin{itemize}[font= \color{blue} \Large, label= $\bullet$]
  \item \textbf{Initialisation} : $r = 1$. Dans ce cas $\Lambda _ { r } =
         \acc{\mu_1} = \conv \prt{\mu_1}$ et l'inclusion est vérifiée.
  \item \textbf{Hérédité} : Soit $r$ tel que $2 \leq r \leq q$. Supposons le résultat acquis pour les
        rangs strictement inférieurs à $r$ et soit $\lambda \in \Lambda _ { r }$. On a :
        $$\frac{\lambda - \lambda_r \cdot \mu_r}{1 - \lambda_r} = \frac{1}{1-\lambda_r}(1-\lambda_r, \lambda_2-\lambda_r, \ldots, \lambda_{r_0}-\lambda_{r}, 0, \ldots, 0)$$
        avec $\displaystyle r_0 = \max \acc{i \in \acc{1 \ldots q} \; \mid \; \lambda_i-\lambda_r>0}$. Notons $\gamma$ le vecteur ci-dessus. On a $\displaystyle \gamma \in \Lambda_{r_0}$. Par hypothèse de récurrence, on peut écrire :
        $$\gamma = \sum_{i=1}^{r_0} \alpha_i \mu_i \quad \text{ avec } \quad
         \sum_{i=1}^{r_0} \alpha_i=1 \quad \text{ et } \quad \alpha_i \geq 0.$$
        D'où
        $$
         \lambda = \prt{1-\lambda_r} \gamma + \lambda_r \mu_r
         = \sum_{i=1}^{r_0} \alpha_i \prt{1-\lambda_r} \mu_i + \lambda_r \mu_r
        $$
        avec $\displaystyle \sum_{i=1}^{r_0} \alpha_i \prt{1-\lambda_r} + \lambda_r=1$ et
        $\alpha_i \prt{1-\lambda_r}$, $\lambda_r$ positifs. D'où le résultat.
 \end{itemize}
\end{rep}

\begin{qst}
 Montrer que, pour toute fonction convexe $f : \mathbb { R } ^ { q } \rightarrow \mathbb { R }$
 et tous $x _ { 1 } , \dots , x _ { r } \in \mathbb { R } ^ { q }$,
 $$\sup _ { \operatorname { conv } \left( x _ { 1 } \ldots \ldots , x _ { r } \right) } f ( x ) = \max _ { j = 1 , \ldots , r } f \left( x _ { j } \right).$$
\end{qst}

\begin{rep}
 Soit $x \in \conv \prt{x_1, \ldots, x_r}$. On peut écrire $x = \sum_{i=1}^{r}
  \alpha_i x_i$ où les $\alpha_i$ désignent des pondérations. En appliquant l'inégalité
 de Jensen, on a :
 $$f\prt{x} = f\prt{\sum_{i=1}^{r} \alpha_i x_i} \leq \sum_{i=1}^{r} \alpha_i f\prt{x_i}
  \leq f\prt{x_{i_0}}$$
 avec $\displaystyle i_0 = \argmax \acc{f\prt{x_i} \; \mid \; i \in \acc{1 \ldots r}}$
 et égalité pour $x = x_{i_0}$. En passant à gauche de l'inégalité au supremum
  lorsque $x$ parcours $\conv \prt{x_1, \ldots, x_r}$, on obtient bien l'égalité
  souhaité.
\end{rep}

\begin{qst}

 Déduire des deux questions précédentes
 $$
  \max _ { \lambda \in \Lambda _ { r } } \max _ { \lambda \in \Lambda _ {
    \bar{r} } } \lambda ^ { \top } ( D - I ) \lbar \leq \max _ { j = 1 , \ldots ,
   r } \max _ { i = 1 , \ldots , \bar{r} } \mu _ { j } ^ { \top } ( D - I )
  \mu _ { i }
 $$
 où $r = \rg{A}$, $\bar{r} = \rg{B}$ et $I$ est la matrice identité $q\times q$.
\end{qst}

\begin{rep}
 Tout d'abord remarquons que l'on peut transformer le membre de gauche dans
 \eqref{eq:D} comme suit :
 $$\sum _ { i , j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { i } ( B )
  d _ { i j } - \sum _ { j = 1 } ^ { q } \lambda _ { j } ( A ) \lambda _ { j } ( B )
  = \sum _ { i = 1 } ^ { r } \sum _ { j = 1 } ^ { \bar{r} } \lambda_i\prt{A} \prt{d_{i,j}-\delta_{i,j}} \lambda_j\prt{B} = \transpose{\lambda\prt{A}} \prt{D-I} \lambda \prt{B}$$
 où $\delta_{i,j}$ désigne le symbole de Kronecker. Notons $f : \Lambda_r\times \Lambda_{\bar{r}} \rightarrow \mathbb { R }$, $(\lambda, \lbar)
  \mapsto \transpose{\lambda} \prt{D-I} \lbar$ qui est bien convexe car linéaire.
 En utilisant $ \displaystyle \Lambda _ { \bar{r} } \subseteq \operatorname { conv }
  \left( \mu _ { 1 } , \ldots , \mu _ { \bar{r} } \right)$ et la question précédente, on a successivement :
 $$\forall \lambda \in \Lambda_r,\; \forall \lbar \in \Lambda_{\bar{r}} \qquad f\prt{\lambda, \lbar} \leq
  \sup_{\lbar \in \Lambda_{\bar{r}}} f\prt{\lambda, \lbar} \leq \sup_{\lbar \, \in \, \conv \acc{\mu_1 \ldots \mu_{\bar{r}}}\, } f\prt{\lambda, \lbar} = \max_{\lbar \,\in \,\acc{\mu_1 \ldots \mu_{\bar{r}}}\,} f\prt{\lambda, \lbar}
  .$$
 En réappliquant le même raisonnement à $g: \lambda \mapsto \max_{\lbar \,\in \,\acc{\mu_1 \ldots \mu_{\bar{r}}}\,} f\prt{\lambda, \lbar}$ qui est bien convexe
 en tant que maximum de fonctions convexes, on a le résultat souhaité.
\end{rep}

\begin{qst}

 Conclure en déduisant l'inégalité \eqref{eq:D} de la question 5 et des propriétés
 de la matrice $D$.
\end{qst}

\begin{rep}

 On a que
 $$\mu _ {j} ^ {\top} ( D - I )\mu _ {i} = \sum_{k = 1}^j \sum_{l=1}^i \prt{d_{k,l} - \delta_{k,l}} = -\sum_{k = 1}^j \prt{1-\sum_{l=1}^i d_{k,l}}
  \leq -\sum_{k = 1}^q \prt{1-\sum_{l=1}^i d_{k,l}} \leq 0$$
 car $1 - \sum_{l=1}^i d_{k,l} \geq 1 - \sum_{l=1}^q d_{k,l} \geq 0$ par la question 1. D'où
 $\displaystyle \max _ { j = 1 , \ldots ,
   r } \max _ { i = 1 , \ldots , \bar{r} } \mu _ { j } ^ { \top } ( D - I )
  \mu _ { i } \leq 0$ et l'on a bien l'inégalité \eqref{eq:D} demandé.
\end{rep}


\begin{exo}
 Soit le modèle $\displaystyle Y = \Aopt + W$ avec $Y$, $\Aopt$, $W$ dans $\R^{
   m\times k}$. On notera par la suite
 $\lambda_j$ et $\lestim_j$ les valeurs singulières des matrices $\Aopt$ et $Y$
 respectivement. On suppose que la matrice $W$ est \ssg.

 Soit l'estimateur \textit{soft-thresholding} matriciel :
 $$\hat { A } ^ { S } = \sum _ { j = 1 } ^ { q } \left( \hat { \lambda } _ { j } ( Y ) - \tau \right) _ { + } \hat { u } _ { j } \hat { v } _ { j } ^ { \top },$$
 avec $\tau >0$  et soit $\delta$ dans $\prt{0,1}$.
 Montrer que l'on a
 $$\Proba{\norme{\Aestim^S - \Aopt}_F^2 \leq C \sigma^2 \rg \Aopt \prt{m+k+\log  \frac{1}{\delta}}} \geq 1- \delta,$$
 avec $C$ une constante absolue et $\tau$ que l'on spécifiera.
\end{exo}

\setcounter{sol}{1}
\begin{sol}

 On se place sur $$\A := \acc{\norme{W}_{\infty} \leq r} \quad \text{ avec } \quad
  r = \sigma \pt{3 \sqrt{\log 3} \sqrt{m+k} + 2 \sqrt{2
    \log \frac{1}{\delta}}}.$$ Par le lemme 6.2, on a $\displaystyle \Proba{\A}
  \geq 1-\delta$.

 En introduisant $\Aestim := \Aestim^S$ pour alléger les notations, on a successivement:

 \begin{alignat}{3}
  \norme{\Aestim - \Aopt}_F^2
   & \leq \enspace \rg \prt{\Aestim - \Aopt}      &\;\cdot
  &\; \norme{\Aestim - \Aopt}_{\infty}^2   \nonumber  \\
   & \leq \prt{\rg \Aestim + \rg \Aopt} \;  &\cdot &\;\norme{\Aestim - \Aopt}_{\infty}^2 \label{eq:rg}
 \end{alignat}
 \begin{itemize}[font= \color{blue} \Large, label= $\bullet$]
  \item Montrons que $\rg \Aestim \leq \rg \Aopt$. On a
        $$\rg \Aestim = \Card \acc{j : \lambda_j\prt{\Aestim}>0} = \Card \acc{j :
          \left( \hat { \lambda } _ { j } - \tau \right) _ { + }>0} =  \Card \acc{j : \lestim_j>\tau}.$$
        Soit $j$ tel que $\lestim_j>\tau$. En utilisant l'inégalité de Weyl, on a
        $$\lambda_j = \lambda_j - \lestim_j+ \lestim_j \geq - \max \abs{\lambda_j
          - \lestim_j} + \lestim_j>-\norme{\Aopt - Y}_{\infty} + \tau =  \tau
         -\norme{W}_{\infty} \geq \tau - r$$
        sur $\A$. En prenant $\tau = r $, on a bien $\lambda_j>0$ et
        $$\rg \Aestim = \Card \acc{j : \lestim_j>\tau} \leq \Card \acc{j : \lambda_j>0} = \rg \Aopt.$$

  \item Majorons le terme de droite dans l'inégalité \eqref{eq:rg}. On a par inégalité
        triangulaire :
        \begin{align*}
         \norme{\Aestim - \Aopt}_{\infty} & \leq \norme{\Aestim - Y}_{\infty} + \norme{Y - \Aopt}_{\infty}                                                             \\
                                          & \leq \norme{\sum_{j=1}^q \prt{\lestim_j- \prt{\lestim_j - \tau}_{+}}\hat{u_j} \transpose{\hat{v_j}} } + \norme{W}_{\infty} \\
                                          & \leq \max_{1 \leq j \leq q} \acc{\lestim_j- \prt{\lestim_j - \tau}_{+}} + r                                                \\
                                          & \leq \tau + r = 2 \, r
        \end{align*}
        en utilisant l'inégalité $0 \leq \lestim_j- \prt{\lestim_j - \tau}_{+} \leq \tau $\footnote{En effet si $\lestim_j \leq \tau$, alors l'expression évaluée vaut $\lestim_j$ qui est bien inférieur à $\tau$. Sinon $\lestim_j > \tau$ et l'expresssion évaluée vaut exactement $\tau$. Pour la positivité, il suffit de constater que $\lambda_j$ et $\tau$ sont tous deux positifs.} et le fait que l'on soit sur $\A$.
 \end{itemize}
 En utilisant l'inégalité $(a+b)^2 \leq 2 (a^2+b^2)$, on a donc sur $\A$ :
 \begin{align*}
  \norme{\Aestim^S - \Aopt}_F^2 & \leq 2 \rg \Aopt \cdot 4 r^2                                                                 \\
                                & = 8 \rg \Aopt \sigma^2 \prt{3 \sqrt{\log 3} \sqrt{m+k} + 2 \sqrt{2 \log \frac{1}{\delta}}}^2 \\
                                & \leq C \rg \Aopt \sigma^2 \prt{m+k+\log \frac{1}{\delta}}
 \end{align*}
 avec $C := 16 \max \acc{9 \log 3, 8}$ constante indépendante des variables du problème.
\end{sol}

\begin{exo}

 Montrer que l'estimtaeur $\Aestim^H$ est solution du problème de minimisation
 \begin{equation}\label{eq:H}
  \min _ { A \in \mathbb { R } ^ { m \times k } } \left( \| Y - A \| _ { F }
  ^ { 2 } + \tau ^ { 2 } \rg A  \right).
 \end{equation}
\end{exo}

\begin{sol}
 Notons $\Aestim$ une solution du problème de minimisation \eqref{eq:H}.
 Développons le premier terme :
 $$\norme{A-Y}_F^2 = \norme{A}_F^2+\norme{Y}_F^2-2 \tr{\transpose{A}Y}.$$
 Le terme $\norme{Y}_F^2$ ne dépendant pas de la variable d'optimisation $A$, on a
 $$
  \Aestim \in \argmin _ { A \in \mathbb { R } ^ { m \times k } } \acc{ \norme{A}_F^2
   -2 \tr{\transpose{A}Y} + \tau ^ { 2 } \rg A }.
 $$
 Notons $ \displaystyle V: \R^{m \times k} \rightarrow \R, A \mapsto \norme{A}_F^2
  -2 \tr{\transpose{A}Y} + \tau ^ { 2 } \rg A$. En exprimant les quantités en terme
 de valeurs singulières et en appliquant l'inégalité de Von Neumann, on a :
 \begin{align*}
  V(A) & \geq  \sum_j
  \lambda_j^2\prt{A} -2 \sum_j \lestim_j \lambda_j \prt{A} + \tau^2 \sum_j \mathbbm{1}_{\lambda_j\prt{A}>0}                                              \\
       & = \sum_j \acc{\lambda_j^2\prt{A} - \lestim_j \lambda_j \prt{A} + \tau^2 \mathbbm{1}_{\lambda_j\prt{A}>0}} \\
       & \geq \min_{t = \prt{t_1 \ldots t_q} \atop t_1 \geq t_2 \geq \ldots
   t_q \geq 0} \sum_j \acc{t_j^2 - \lestim_j t_j + \tau^2 \mathbbm{1}_{t_j>0}}                                     \\
       & \geq \min_{t_j \geq 0} \sum_j \acc{t_j^2 - \lestim_j t_j + \tau^2 \mathbbm{1}_{t_j>0}}.
 \end{align*}
 On notera que le passage de l'avant-dernière inégalité à la dernière est en
 réalité une égalité, la valeur de la somme  ne dépendant pas de
 l'indexation choisie pour les $t_j$.


 Introduisons la fonction réelle $g(x) = x^2-2 \lestim x + \tau^2 \mathbbm{1}_{x>0}$ définie sur $\R_{+}$.
 On peut réécrire $g$ comme
 $$ g(x)=
  \begin{dcases}
   \; \prt{x-\lestim}^2 + \tau^2-\lestim^2 & \text{ si x>0}    \\
   \; 0                                    & \text{ si } x = 0
  \end{dcases}
 $$
 \begin{itemize}[font= \color{blue} \Large, label= $\bullet$]
  \item Si $\tau^2>\lestim^2$\footnote{\ie \; $ \displaystyle \tau>\lestim$ puisque les deux termes sont positifs.},  alors $g>0$ sur $\R_{+}^{\star}$ d'où $x = 0$ est
        la seule solution qui minimise $g$.
  \item Si $\tau^2 = \lestim^2$, alors les valeurs optimales sont $x = 0$ et $x = \lestim$.
  \item Si $\tau^2< \lestim^2$, alors $g$ est négative strictement sur l'intervalle
        $(\lestim-\sqrt{\lestim^2-\tau^2}, \lestim+\sqrt{\lestim^2-\tau^2})$
        avec un minimum atteint en $x = \lestim$.
 \end{itemize}
 Finalement l'estimateur de \textit{hard-thresholding} matricielle définie par
 $$\Aestim^H =  \sum _ { j = 1 } ^ { q }  \lestim_j I\prt{\lestim>\tau}
  \hat { u } _ { j } \hat { v } _ { j } ^ { \top }$$
 a bien ses valeurs singulières vérifiant les conditions d'optimalités ci-dessus,
 d'où le résultat.
\end{sol}
